{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6185873-d4f8-4a08-a157-e0b686883522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,confusion_matrix\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dbc0321-aa74-44f7-a9a7-fdc59e76c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    class_names = sorted(os.listdir(dataset_path))\n",
    "\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    image_path = os.path.join(class_path, filename)\n",
    "                    image = cv2.imread(image_path)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    image = np.array(image)\n",
    "                    image = image / 255.0\n",
    "                    if image is not None:\n",
    "                        x_train.append(image)\n",
    "                        y_train.append(class_index)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e253d51-cee9-4588-83dd-c80a0ceb5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    # Load the MobileNetV2 model with pre-trained weights on ImageNet\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet', \n",
    "        include_top=False, \n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Add a dense layer with 128 neurons and ReLU activation\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    \n",
    "    # Add a final dense layer with 4 neurons (one for each class) and softmax activation\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create the new model with the modified architecture\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Freeze the weights of the base model to avoid overfitting on small datasets\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11b5a1f1-b6c4-4dd1-956a-c7586efc36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "num_classes = 4\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3220a3c1-753b-4f37-90e3-ebd7ec459338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 224, 224, 3), (426,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset directory\n",
    "dataset_directory = \"data/MSID_US\"\n",
    "\n",
    "# Load dataset into x_train and y_train arrays\n",
    "dataset_x_train, dataset_y_train = load_dataset(dataset_directory)\n",
    "\n",
    "dataset_x_train.shape, dataset_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf456694-2cf4-4875-ad75-5cbd7fb1bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving loaded dataset into images and labels numpy arrays for later re-use\n",
    "dataset_images_array_name = 'data/MSID_AUG_IMAGES_ARRAY.npy'\n",
    "dataset_labels_array_name = 'data/MSID_AUG_LABELS_ARRAY.npy'\n",
    "\n",
    "np.save(dataset_images_array_name, dataset_x_train);\n",
    "np.save(dataset_labels_array_name, dataset_y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b0e9-9fce-48d7-a5f9-0aa58d9b94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved numpy dataset\n",
    "dataset_x_train = np.load(dataset_images_array_name)\n",
    "dataset_y_train = np.load(dataset_labels_array_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5488380a-779f-4086-98b8-9d349f0f37df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((306, 224, 224, 3), (77, 224, 224, 3), (43, 224, 224, 3))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset_x_train, dataset_y_train, test_size=0.1)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "# Reshaping labels to (x, 4)\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_one_hot = tf.keras.utils.to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "x_train.shape, x_val.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7343f657-19d4-41c8-bc58-1943708705eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 4s 282ms/step - loss: 1.0908 - accuracy: 0.5131 - val_loss: 0.7087 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 2s 251ms/step - loss: 0.4422 - accuracy: 0.8595 - val_loss: 0.6132 - val_accuracy: 0.7792 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 3s 261ms/step - loss: 0.2760 - accuracy: 0.9150 - val_loss: 0.5371 - val_accuracy: 0.8312 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 2s 250ms/step - loss: 0.1747 - accuracy: 0.9641 - val_loss: 0.5631 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 2s 244ms/step - loss: 0.1147 - accuracy: 0.9869 - val_loss: 0.5323 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 2s 239ms/step - loss: 0.0834 - accuracy: 0.9935 - val_loss: 0.5462 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 2s 242ms/step - loss: 0.0560 - accuracy: 0.9935 - val_loss: 0.5349 - val_accuracy: 0.8052 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 2s 247ms/step - loss: 0.0427 - accuracy: 0.9967 - val_loss: 0.5509 - val_accuracy: 0.8442 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 2s 242ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.5440 - val_accuracy: 0.8312 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 2s 241ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.5408 - val_accuracy: 0.8442 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 2s 243ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.5526 - val_accuracy: 0.8442 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 3s 258ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.5478 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 2s 243ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.5486 - val_accuracy: 0.8701 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 2s 244ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.5483 - val_accuracy: 0.8571 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 2s 242ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.5495 - val_accuracy: 0.8571 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 2s 252ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.5505 - val_accuracy: 0.8571 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 2s 247ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.5513 - val_accuracy: 0.8571 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.5516 - val_accuracy: 0.8701 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 2s 243ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.5522 - val_accuracy: 0.8701 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 2s 242ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.5522 - val_accuracy: 0.8701 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a0575d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Set up the learning rate schedule\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7)\n",
    "\n",
    "# Train the model using fit_generator with the learning rate schedule\n",
    "model.fit(\n",
    "    x_train, \n",
    "    y_train_one_hot,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val_one_hot), \n",
    "    callbacks=[reduce_lr], \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63dd0e-83e7-4842-91fc-8c78c5815690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dceef96-1108-4c8a-b79b-986c59256942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 98ms/step\n",
      "Metrics:\n",
      "Precision: 0.8846153846153846\n",
      "Recall: 0.8794642857142857\n",
      "F1 Score: 0.8574074074074074\n",
      "Accuracy: 0.8604651162790697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict on the validation data\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate the metrics\n",
    "precision = precision_score(y_test, y_pred_labels, average='macro')\n",
    "recall = recall_score(y_test, y_pred_labels, average='macro')\n",
    "f1 = f1_score(y_test, y_pred_labels, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Metrics:\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea89f95c-a936-4d7f-a3d2-3e07dc1bcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/mobilenetv2_original_split_validation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafe905f-83d9-436f-8e4a-d1a9e6dc7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/mobilenetv2_optimizer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f8c83-e769-41df-928e-1a90f8752de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation data\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)  \n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['Chickenpox', 'Measles', 'Monkeypox', 'Normal']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_labels))\n",
    "plt.xticks(tick_marks, class_labels, rotation=45)\n",
    "plt.yticks(tick_marks, class_labels)\n",
    "\n",
    "# Add values to each cell of the matrix\n",
    "thresh = cm.max() / 2.0\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Mostrar el plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f165761-c9c3-45d6-ad1d-51e210ac23ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 14:51:07.317641: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 313ms/step\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('test-images/normal-test.jpeg')\n",
    "\n",
    "# Resize the image to the input size expected by the model\n",
    "input_size = (224, 224)  # example input size for a model\n",
    "image = cv2.resize(image, input_size)\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "image = np.array(image)\n",
    "\n",
    "# Scale the pixel values to be between 0 and 1\n",
    "image = image / 255.0\n",
    "\n",
    "# Add an extra dimension to the array to represent the batch size (1 in this case)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "image.shape\n",
    "\n",
    "prediction = model.predict(image)\n",
    "\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81edf63b-a44a-4426-86f1-9fc4756faa32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679c34d-4061-4847-bc78-4edb70ec5ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
